<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Back Propagation &mdash; IntroductionToBackpropagationWithGPU 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="IntroductionToBackpropagationWithGPU 0.1 documentation" href="index.html" />
    <link rel="prev" title="汎用GPUにおける結合荷重及び関連値の確保と保持" href="AllocateMemory4GPGPU.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="AllocateMemory4GPGPU.html" title="汎用GPUにおける結合荷重及び関連値の確保と保持"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">IntroductionToBackpropagationWithGPU 0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="back-propagation">
<h1>Back Propagation<a class="headerlink" href="#back-propagation" title="Permalink to this headline">¶</a></h1>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">題名:</th><td class="field-body">Deep Learning(Neural Network)における Back propagation(逆伝搬)の解説</td>
</tr>
<tr class="field-even field"><th class="field-name">著者:</th><td class="field-body">柏木 明博</td>
</tr>
<tr class="field-odd field"><th class="field-name">作成日:</th><td class="field-body">2017年6月20日</td>
</tr>
</tbody>
</table>
<div class="section" id="id1">
<h2>複数の層を超える誤差の伝搬方法<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Forward Propagation(順伝搬)では、入力層から出力層に向かって、値と荷重の総和
を伝搬して行き、出力層で結果を得るものでしたが、それはその時の結合荷重による
ものでした。Deep Learing(Neural Network)では、学習と言う段階を経て、入力値に
対応した出力値を憶えさせます。つまり、入力値に対応した出力値が得られるように、
結合荷重を調整します。Neural Networkは、生物の神経細胞を模倣したものですから、
結合荷重の調整方法も生物から模倣したいところですが、現在のところ生物がどのよ
うに結合荷重を調整しているのか、正確なところは判明していません。Neural Netwo
rkが発見された初期の頃、パーセプトロンと言うモデルが利用されました。これは、
入力値をForward Propagation(順伝搬)を用いて計算し、得られた結果と、正しい答え
を比較し、その差分を結合荷重に反映するものです。つまり、正しい答えとの誤差が
なくなるように結合荷重を調整して行きます。具体的には、下記のようになります。</p>
<div class="math" id="equation-パーセプトロンの学習方法">
<span class="eqno">(1)</span>\[w = w + \eta ( t - z ) \cdot z\]\[\eta:学習率\]\[t:教師信号\]\[z:出力値\]\[w:結合荷重\]</div>
<p>意外に素直な理解が得られるのではないでしょうか。正しい答えと、間違った答えを
比較して、その誤差を結合荷重に加えて行きます。この方法でも十分有用な利用が可
能ですが、いくつか問題点も見つかっています。それは、線形な情報にしか対応でき
ないことと、誤差を複数の層へ伝えられない事です。線形な情報とは、直線でしか分
離できない情報のことですが、定規で真っ直ぐな線を引いて分離できる情報です。曲
線を使わないと分離できないような、複雑な情報は正確に対応できません。また、誤
差を複数の層へ伝えられないとは、上記(1)の式のように誤差は、正しい答えとの差分
ですから、出力層に置いては正しい答えと現在の出力値を比較することができますが、
出力層以外の中間層や入力層では、比較ができません。これは、実際の生物の神経細
胞においても未だ解明されていない仕組みです。しかし、生物の神経細胞も確かに多
層構造となっており、何らかの伝達物質あるいは、伝達方法があるはずだと言われて
います。実際、その伝達物質や伝達方法が見つかった、と言うニュースが時々流れて
いますが、確証されてはいないようです。</p>
<p>そこで考案されたのが、確率的勾配降下法によるBackpropagation(誤差逆伝搬)です。
確率的勾配降下法では、上記パーセプトロンによる学習方法と同じように正しい答え
との誤差を用いて結合荷重を更新して行きますが、更新には誤差に対する結合荷重に
よる微分値、つまり一階微分ですから、傾きを用いて更新して行きます。傾きがプラ
ス方向の場合は、結合荷重をマイナス方向へ、傾きがマイナス方向の場合は、結合荷
重をプラス方向へ更新します。式は、パーセプトロンの学習方法と基本的な考え方は
変わっておらず、これまでの言葉による説明を式にすると、下記のようになります。</p>
<div class="math" id="equation-確率的勾配降下法による学習方法">
<span class="eqno">(2)</span>\[w = w + ( -\epsilon \Delta E )\]\[\epsilon:学習率\]\[E:誤差値\]\[z:前層出力\]\[w:結合荷重\]</div>
<p>そして、この結合荷重の更新を出力層から、入力層に向かって遡ってゆくことから
Backpropagation(誤差逆伝搬)と呼ばれます。誤差値はEで表しましたが、遡る誤差は、
誤差信号 <span class="math">\(\delta\)</span> と表し、以下のように計算します。</p>
<div class="math" id="equation-誤差信号d">
<span class="eqno">(3)</span>\[\delta = ( w \cdot {\delta}_{k} ) \cdot f'(z)\]\[f'(z) = \{ 1 - f( z ) \} \cdot f(z)\]\[f:シグモイド関数\]\[f':微分したシグモイド関数\]\[{\delta}_{k}:誤差信号（出力層側）\]\[z:前層出力（入力層側）\]\[w:結合荷重\]</div>
<p><span class="math">\(f'\)</span> は、前の項目「Forward Propagation」で解説しているシグモイド関数を
微分したものです。出力層の <span class="math">\(\delta\)</span> だけは下記の式によって得ます。</p>
<div class="math" id="equation-出力層の誤差信号d">
<span class="eqno">(4)</span>\[\delta = ( z - t )\]\[z:出力層出力\]\[t:教師信号\]</div>
<p>この <span class="math">\(\delta\)</span> から初めて、中間層(出力層側)から順番に入力層側へ <span class="math">\(\delta\)</span>
を計算して行きます。具体的なコードで表すと、以下のようになります。</p>
<p>LIST 1. 引数取得</p>
<div class="highlight-c"><div class="highlight"><pre><span></span><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">calc_delta</span><span class="p">(</span>
        <span class="kt">long</span> <span class="n">trg</span><span class="p">,</span>
        <span class="kt">long</span> <span class="n">unitk</span><span class="p">,</span>
        <span class="kt">long</span> <span class="n">unitj</span><span class="p">,</span>
        <span class="kt">long</span> <span class="n">l_num</span><span class="p">,</span>
        <span class="kt">void</span> <span class="o">*</span><span class="n">mem</span>
<span class="p">){</span>
</pre></div>
</div>
<p>LIST 2. 変数宣言</p>
<div class="highlight-c"><div class="highlight"><pre><span></span><span class="kt">int</span> <span class="n">tid</span><span class="p">;</span>
                                <span class="c1">// thread id</span>
<span class="kt">long</span> <span class="n">k_cnt</span><span class="p">;</span>
                                <span class="c1">// counter of output side</span>
<span class="kt">long</span> <span class="n">j_cnt</span><span class="p">;</span>
                                <span class="c1">// counter of input side</span>
<span class="kt">double</span> <span class="n">ff</span><span class="p">;</span>
                                <span class="c1">// number of differential s</span>
<span class="kt">double</span> <span class="n">sum</span><span class="p">;</span>
                                <span class="c1">// number of summary</span>
<span class="n">NEURON_T</span> <span class="o">*</span><span class="n">n</span><span class="p">;</span>
                                <span class="c1">// pointer of neuron</span>
<span class="kt">long</span> <span class="n">kphase</span><span class="p">;</span>
                                <span class="c1">// number of output side phase</span>
<span class="kt">long</span> <span class="n">jphase</span><span class="p">;</span>
                                <span class="c1">// number of input side phase</span>
<span class="kt">long</span> <span class="n">j</span><span class="p">;</span>
                                <span class="c1">// counter of j phase</span>
</pre></div>
</div>
<p>LIST 3. GPUに関連した処理</p>
<div class="highlight-c"><div class="highlight"><pre><span></span><span class="n">tid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="k">if</span><span class="p">(</span><span class="n">tid</span> <span class="o">&gt;</span> <span class="n">unitj</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">||</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">){</span>
                                <span class="c1">// check for enable threads</span>
        <span class="k">return</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>nVIDIA GPU CUDA Cにおける定形処理のようなものですが、実際に実行される
threadは、jユニットごとに一つとなる為、CUDAが呼び出したthread番号が
jユニットに対応していないthreadは、何もせずに処理を返します。</p>
<p>LIST 4. 直線的なメモリ領域から、jの位置を求める関数</p>
<div class="highlight-c"><div class="highlight"><pre><span></span><span class="n">__device__</span> <span class="n">__host__</span> <span class="kt">long</span> <span class="nf">calcj</span><span class="p">(</span> <span class="kt">long</span> <span class="n">j</span><span class="p">,</span> <span class="kt">long</span> <span class="n">jmax</span><span class="p">,</span> <span class="kt">long</span> <span class="n">k</span> <span class="p">){</span>

        <span class="k">return</span> <span class="n">j</span> <span class="o">+</span> <span class="p">(</span><span class="n">jmax</span> <span class="o">*</span> <span class="n">k</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>LIST 5. <span class="math">\(\delta\)</span> の計算</p>
<div class="highlight-c"><div class="highlight"><pre><span></span>                                <span class="c1">// set neuron instance</span>
<span class="n">n</span> <span class="o">=</span> <span class="p">(</span><span class="n">NEURON_T</span> <span class="o">*</span><span class="p">)</span><span class="n">mem</span><span class="p">;</span>
                                <span class="c1">// set phase number</span>
<span class="n">jphase</span> <span class="o">=</span> <span class="n">trg</span> <span class="o">+</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">kphase</span> <span class="o">=</span> <span class="n">trg</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
                                <span class="c1">// set number of unit</span>
<span class="n">unitj</span> <span class="o">=</span> <span class="n">n</span><span class="o">-&gt;</span><span class="n">z_num</span><span class="p">[</span><span class="n">jphase</span><span class="p">];</span>
<span class="n">unitk</span> <span class="o">=</span> <span class="n">n</span><span class="o">-&gt;</span><span class="n">z_num</span><span class="p">[</span><span class="n">kphase</span><span class="p">];</span>

                                <span class="c1">// set block id</span>
<span class="n">j_cnt</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="k">if</span><span class="p">(</span><span class="n">j_cnt</span> <span class="o">&lt;</span> <span class="n">unitj</span><span class="p">){</span>
                                <span class="c1">// calculate forward</span>
        <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

        <span class="n">ff</span> <span class="o">=</span> <span class="n">differented_sigmoid</span><span class="p">(</span> <span class="n">n</span><span class="o">-&gt;</span><span class="n">z</span><span class="p">[</span><span class="n">jphase</span><span class="p">][</span><span class="n">j_cnt</span><span class="p">]</span> <span class="p">);</span>

        <span class="k">for</span><span class="p">(</span> <span class="n">k_cnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k_cnt</span> <span class="o">&lt;</span> <span class="n">unitk</span><span class="p">;</span> <span class="n">k_cnt</span><span class="o">++</span> <span class="p">){</span>

                <span class="n">j</span> <span class="o">=</span> <span class="n">calcj</span><span class="p">(</span> <span class="n">j_cnt</span><span class="p">,</span> <span class="n">unitj</span><span class="p">,</span> <span class="n">k_cnt</span> <span class="p">);</span>

                <span class="n">sum</span> <span class="o">+=</span> <span class="n">n</span><span class="o">-&gt;</span><span class="n">w</span><span class="p">[</span><span class="n">kphase</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">[</span><span class="n">kphase</span><span class="p">][</span><span class="n">k_cnt</span><span class="p">]</span> <span class="o">*</span> <span class="n">ff</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="n">n</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">[</span><span class="n">jphase</span><span class="p">][</span><span class="n">j_cnt</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>

        <span class="n">n</span><span class="o">-&gt;</span><span class="n">db</span><span class="p">[</span><span class="n">jphase</span><span class="p">][</span><span class="n">j_cnt</span><span class="p">]</span>
                <span class="o">=</span> <span class="n">n</span><span class="o">-&gt;</span><span class="n">db</span><span class="p">[</span><span class="n">jphase</span><span class="p">][</span><span class="n">j_cnt</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span><span class="o">-&gt;</span><span class="n">b</span><span class="p">[</span><span class="n">jphase</span><span class="p">][</span><span class="n">j_cnt</span><span class="p">]</span> <span class="o">*</span> <span class="n">ff</span><span class="p">;</span>
<span class="p">}</span>
                                <span class="c1">// Normal return</span>
<span class="k">return</span><span class="p">;</span>
</pre></div>
</div>
<p>こうして計算した各層の <span class="math">\(\delta\)</span> と式(2)を用いて、各層の結合荷重wを更新します。</p>
<p>これは、一般的な総和の式ですが、入力される信号 <span class="math">\(x\)</span> に荷重 <span class="math">\(w\)</span> を
掛けて、その総計をとり、バイアスを足すというものです。プログラミング言語(C言
語)で表すと、このようになります。</p>
<div class="highlight-c"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

<span class="k">for</span><span class="p">(</span> <span class="n">i_cnt</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i_cnt</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i_cnt</span><span class="o">++</span> <span class="p">){</span>

        <span class="n">z</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">i_cnt</span><span class="p">][</span><span class="n">j_cnt</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i_cnt</span><span class="p">];</span>
<span class="p">}</span>

<span class="n">z</span> <span class="o">+=</span> <span class="n">b</span><span class="p">[</span><span class="n">j_cnt</span><span class="p">];</span>
</pre></div>
</div>
<p>プログラムで表すと、さほどの事はなく、単に集計しているだけなのがよく分かります。
神経細胞を模式的に表すと、下記のようになります。左の図は、実際の神経細胞、右の
図は、その数理モデルの模式図です。つまり、複数の入力を受取り、一つの出力を次の
神経細胞に渡す生きたデバイスです。</p>
<div class="figure align-center">
<img alt="神経細胞とモデル" src="neuron_model.png" />
<p class="caption">図1.神経細胞とモデル</p>
</div>
<p>この図を例に説明すると、前の神経細胞からの入力値 <span class="math">\(x_1\)</span> 〜 <span class="math">\(x_3\)</span> に、
それぞれの結合荷重 <span class="math">\(w_1\)</span> 〜 <span class="math">\(w_3\)</span> を掛けた値を合計し、それにバイア
スを加えます。次の神経細胞へ信号を伝達するには一定の条件があり、この総和がある
値（閾値 <span class="math">\(θ\)</span> ）を超えると出力 <span class="math">\(z\)</span> が <span class="math">\(1\)</span> となります。この総和
を <span class="math">\(u\)</span> とすると、出力 <span class="math">\(z\)</span> は以下の関数 <span class="math">\(z=f(u)\)</span> となります。</p>
<div class="math" id="equation-閾値による出力値">
<span class="eqno">(5)</span>\[\begin{split}f(u) =\begin{cases}1 &amp; u &gt;=  \theta \\0 &amp; u &lt; \theta \end{cases}\end{split}\]\[θ:閾値\]</div>
<p>Neural Networkにおける「学習」処理とは、目的の出力が得られるように、結合荷重
<span class="math">\(w\)</span> をさまざまに変化させる処理を指します。この結合荷重を変化させることで、
入力に対する出力を変化・決定させることができます。</p>
<p>神経細胞の組合せで、動物は情報処理を行っているわけですが、この一つの方式のデバ
イスで、現在のノイマン型コンピュータを構成しているディジタル回路（論理積・論理
和・排他的論理和等）をすべて表すことができます。つまり、動物の神経細胞でいま私
達が利用しているコンピュータを作成することができます。コンピュータにできること
は、動物の脳でも可能なのです。</p>
</div>
<div class="section" id="id2">
<h2>活性化関数<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>活性化関数は、細胞やたんぱく質の反応をモデル化したものですが、Neural Networkで
は出力値 <span class="math">\(z\)</span> を実際の神経細胞の出力と考えられる形に近づけるために用いられ
ます。一般に、ミカエリス・メンテン式によく似た非線形関数であるシグモイド関数や
tanh関数、また出力が線形になる正規化線形関数などがあります。今回は、活性化関数
の基本として受け入れられているシグモイド関数を用いています。</p>
<div class="math" id="equation-シグモイド関数">
<span class="eqno">(6)</span>\[f(x) = \frac{1}{1+ e^{- \alpha x} }\]\[x:入力\]\[e:自然対数\]\[α:定数\]</div>
<p>シグモイド関数は、非線形の関数であり、入力の値域を <span class="math">\(-∞\)</span> 〜 <span class="math">\(+∞\)</span> に
取る、出力 <span class="math">\(0\)</span> 〜 <span class="math">\(1\)</span> の関数です。出力は下記のようになります。シグ
モイドとは、アルファベットのSの意味です。</p>
<div class="figure align-center">
<img alt="シグモイド曲線" src="sigmoid.png" />
<p class="caption">図2.シグモイド曲線</p>
</div>
<p>注意しなければいけないのは、入力の値域が無限大とはいえ、その可変域は限られる点
です。大きすぎる値や小さすぎる値を入力しても、 <span class="math">\(1\)</span> または <span class="math">\(0\)</span> の出
力しか得られなくなります。定数 <span class="math">\(α\)</span> によってシグモイド曲線の形を変えること
が出来るため、入力値の最大・最小値に合わせて <span class="math">\(α\)</span> を変更するか、入力値を正
規化するなどして調整します。</p>
</div>
<div class="section" id="id3">
<h2>論理積と論理和による実際の計算例<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>論理積と論理和の計算を例に、実際にForward Propagation(順伝搬)を計算してみます。
論理積の入力と出力(真理値表)は、以下の通りです。</p>
<table border="1" class="docutils">
<caption>論理積(AND)</caption>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head stub">&nbsp;</th>
<th class="head"><blockquote class="first last">
<div><span class="math">\(x_1\)</span></div></blockquote>
</th>
<th class="head"><blockquote class="first last">
<div><span class="math">\(x_2\)</span></div></blockquote>
</th>
<th class="head"><blockquote class="first last">
<div><span class="math">\(z\)</span></div></blockquote>
</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><th class="stub">入力1:</th>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="row-odd"><th class="stub">入力2:</th>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="row-even"><th class="stub">入力3:</th>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="row-odd"><th class="stub">入力4:</th>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>そして、結合荷重 <span class="math">\(w_1=0.53\)</span> と <span class="math">\(w_2=0.33\)</span>、閾値 <span class="math">\(θ=0.7\)</span> とす
ると、式1と式2を上記の表のそれぞれの値について計算した結果は、以下のようになり
ます。</p>
<table border="1" class="docutils">
<caption>論理積(AND)の計算結果</caption>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head stub">&nbsp;</th>
<th class="head">式1:( <span class="math">\(x_1 w_1)+(x_2 w_2)\)</span></th>
<th class="head">式2:閾値との関係</th>
<th class="head"><blockquote class="first last">
<div><span class="math">\(z\)</span></div></blockquote>
</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><th class="stub">入力1:</th>
<td><span class="math">\((0*0.53)+(0*0.33)=0.00\)</span></td>
<td><span class="math">\(&lt; 0.7\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
<tr class="row-odd"><th class="stub">入力2:</th>
<td><span class="math">\((0*0.53)+(1*0.33)=0.33\)</span></td>
<td><span class="math">\(&lt; 0.7\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
<tr class="row-even"><th class="stub">入力3:</th>
<td><span class="math">\((1*0.53)+(0*0.33)=0.53\)</span></td>
<td><span class="math">\(&lt; 0.7\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
<tr class="row-odd"><th class="stub">入力4:</th>
<td><span class="math">\((1*0.53)+(1*0.33)=0.86\)</span></td>
<td><span class="math">\(&gt; 0.7\)</span></td>
<td><span class="math">\(1\)</span></td>
</tr>
</tbody>
</table>
<p>閾値 <span class="math">\(θ\)</span> 以下の場合は <span class="math">\(z=0\)</span> 、閾値 <span class="math">\(θ\)</span> 以上の場合は <span class="math">\(z=1\)</span>
となります。また、論理和の入力と出力(真理値表)は、以下の通りです。</p>
<table border="1" class="docutils">
<caption>論理和(OR)</caption>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head stub">&nbsp;</th>
<th class="head"><blockquote class="first last">
<div><span class="math">\(x_1\)</span></div></blockquote>
</th>
<th class="head"><blockquote class="first last">
<div><span class="math">\(x_2\)</span></div></blockquote>
</th>
<th class="head"><blockquote class="first last">
<div><span class="math">\(z\)</span></div></blockquote>
</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><th class="stub">入力1:</th>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="row-odd"><th class="stub">入力2:</th>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="row-even"><th class="stub">入力3:</th>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="row-odd"><th class="stub">入力4:</th>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>同じように、結合荷重 <span class="math">\(w_1=0.85\)</span> と <span class="math">\(w_2=0.85\)</span> 、閾値 <span class="math">\(θ=0.7\)</span>
とすると、式1と式2を上記の表のそれぞれの値について計算した結果は、以下のように
なります。</p>
<table border="1" class="docutils">
<caption>論理和(OR)の計算結果</caption>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head stub">&nbsp;</th>
<th class="head">式1:( <span class="math">\(x_1 w_1)+(x_2 w_2)\)</span></th>
<th class="head">式2:閾値との関係</th>
<th class="head"><blockquote class="first last">
<div><span class="math">\(z\)</span></div></blockquote>
</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><th class="stub">入力1:</th>
<td><span class="math">\((0*0.85)+(0*0.85)=0.00\)</span></td>
<td><span class="math">\(&lt; 0.7\)</span></td>
<td><span class="math">\(0\)</span></td>
</tr>
<tr class="row-odd"><th class="stub">入力2:</th>
<td><span class="math">\((0*0.85)+(1*0.85)=0.85\)</span></td>
<td><span class="math">\(&gt; 0.7\)</span></td>
<td><span class="math">\(1\)</span></td>
</tr>
<tr class="row-even"><th class="stub">入力3:</th>
<td><span class="math">\((1*0.85)+(0*0.85)=0.85\)</span></td>
<td><span class="math">\(&gt; 0.7\)</span></td>
<td><span class="math">\(1\)</span></td>
</tr>
<tr class="row-odd"><th class="stub">入力4:</th>
<td><span class="math">\((1*0.85)+(1*0.85)=1.70\)</span></td>
<td><span class="math">\(&gt; 0.7\)</span></td>
<td><span class="math">\(1\)</span></td>
</tr>
</tbody>
</table>
<p>このように、結合荷重を適切に決めることによって、Neural Networkはさまざまな出力を
得ることができます。この計算例では、シグモイド関数(式3)は省略しましたが、上記の
閾値と比較する前の計算結果に関数を適用することで、求めることができます。</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Back Propagation</a><ul>
<li><a class="reference internal" href="#id1">複数の層を超える誤差の伝搬方法</a></li>
<li><a class="reference internal" href="#id2">活性化関数</a></li>
<li><a class="reference internal" href="#id3">論理積と論理和による実際の計算例</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="AllocateMemory4GPGPU.html"
                        title="previous chapter">汎用GPUにおける結合荷重及び関連値の確保と保持</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/BackPropagation.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="AllocateMemory4GPGPU.html" title="汎用GPUにおける結合荷重及び関連値の確保と保持"
             >previous</a> |</li>
        <li><a href="index.html">IntroductionToBackpropagationWithGPU 0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2017, Akihiro Kashiwagi.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.3.
    </div>
  </body>
</html>